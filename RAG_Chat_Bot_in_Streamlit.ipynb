{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainiakhil/RAG-Chat-Bot-Streamlit/blob/main/RAG_Chat_Bot_in_Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C3K5QAag8Y5"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-gpu\n",
        "!pip install streamlit\n",
        "!pip install PyPDF2\n",
        "!pip install pyngrok\n",
        "!pip install bitsandbytes\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "ZEgu2767bdEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "suzYrTeuf9nD"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "8Qn8dXGk3QGr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SC21q7yHvvAy"
      },
      "outputs": [],
      "source": [
        "# Set your ngrok authentication token\n",
        "ngrok.set_auth_token(\"token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeoSs6cYw9dY",
        "outputId": "22887901-4eba-4ebb-f32e-6f2aceab10a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "import PyPDF2\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0,\n",
        "    llm_int8_has_fp16_weight=False,\n",
        ")\n",
        "\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "        device_map = device,\n",
        "        quantization_config = bnb_config,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "# FAISS index and response map global variables\n",
        "index = None\n",
        "response_map = {}\n",
        "\n",
        "st.title(\"Interactive RAG-Based QA Bot\")\n",
        "st.write(\"Upload documents and ask questions in real time!\")\n",
        "\n",
        "uploaded_files = st.file_uploader(\"Upload Documents\", accept_multiple_files=True)\n",
        "\n",
        "\n",
        "if uploaded_files:\n",
        "  documents = []\n",
        "  for uploaded_file in uploaded_files:\n",
        "\n",
        "    pdf_reader = PyPDF2.PdfReader(uploaded_file)\n",
        "    pdf_text = \"\"\n",
        "\n",
        "    # Extract text from each page\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "      page = pdf_reader.pages[page_num]\n",
        "      pdf_text += page.extract_text()\n",
        "\n",
        "    # Split the PDF content into chunks for better embedding\n",
        "    documents = [pdf_text[i:i+1000] for i in range(0, len(pdf_text), 1000)]\n",
        "\n",
        "\n",
        "    # Generate embeddings for uploaded documents\n",
        "    vectors = embedding_model.encode(documents)\n",
        "    vectors = np.array(vectors, dtype=np.float32)\n",
        "\n",
        "    # Initialize FAISS index\n",
        "    dimension = vectors.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(vectors))\n",
        "\n",
        "    response_map = {i: documents[i] for i in range(len(documents))}\n",
        "\n",
        "    st.success(\"Documents uploaded and indexed successfully!\")\n",
        "\n",
        "\n",
        "# Step 2: Query Input and Processing\n",
        "query = st.text_input(\"Enter your question\")\n",
        "\n",
        "if st.button(\"Get Answer\") and query and index is not None:\n",
        "\n",
        "    query_vector = embedding_model.encode([query])\n",
        "    query_vector = np.array(query_vector, dtype=np.float32)\n",
        "\n",
        "      # Perform FAISS search\n",
        "    k = 3  # Number of nearest neighbors to retrieve\n",
        "    distances, indices = index.search(np.array(query_vector), k)\n",
        "\n",
        "    if indices is not None and len(indices) > 0:\n",
        "        retrieved_doc = response_map[indices[0][0]]\n",
        "\n",
        "        augmented_input = query + \"\\n\\n Retrieved Information:\\n\" + retrieved_doc + \"\\n\\n Final Answer: \\n\"\n",
        "        inputs = tokenizer(augmented_input, return_tensors=\"pt\", truncation=True, max_length=500)\n",
        "\n",
        "        output = llm_model.generate(**inputs, max_new_tokens=500, temperature=0.5, top_p=0.85)\n",
        "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "          # Display the result\n",
        "        st.write(\"Answer:\")\n",
        "        st.write(response)\n",
        "    else:\n",
        "      st.error(\"No relevant document found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxLXg2vewP2_"
      },
      "outputs": [],
      "source": [
        "# Start ngrok to expose the Streamlit app to the public\n",
        "public_url = ngrok.connect(addr='8501', proto = 'http',bind_tls = True)\n",
        "print(f'Streamlit app will be live at: {public_url}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwzsDstDUUlx"
      },
      "outputs": [],
      "source": [
        "# Run Streamlit app\n",
        "!streamlit run app.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMJGKQT55oKDCOA5wEpN4+l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}